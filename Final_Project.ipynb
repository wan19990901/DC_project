{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5NDXrN2CtH7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIpCl4lKHY8D",
    "outputId": "e0d2c868-376c-446b-e0c2-674c0f4fb0fb"
   },
   "outputs": [],
   "source": [
    "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/Industrial_and_Scientific.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb5fGi2wJpXI",
    "outputId": "8942d902-3d6b-4a33-b775-fab14abf6754"
   },
   "outputs": [],
   "source": [
    "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/metaFiles2/meta_Industrial_and_Scientific.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBfzThdejDSJ",
    "outputId": "bc623f11-b80f-4da9-e6ca-a251c3080133"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddVjvBQ5BnXq"
   },
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8Qexuq_3Rx0"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"readGZ\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.default.parallelism\", 24) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 24) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the gzipped JSON file directly into a DataFrame\n",
    "df = spark.read.json(\"Industrial_and_Scientific.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSkWP10AtaKv",
    "outputId": "dcdcffe3-6145-4b91-e530-f0b0eafbffff"
   },
   "outputs": [],
   "source": [
    "# Show the DataFrame to check if it's loaded correctly\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7igYuRaV4bF7"
   },
   "outputs": [],
   "source": [
    "### load the meta data\n",
    "\n",
    "df_meta = spark.read.json(\"meta_Industrial_and_Scientific.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vv5iGqYNiq9L",
    "outputId": "82ca18b9-5d7a-4e97-9d9e-11c1593cc75d"
   },
   "outputs": [],
   "source": [
    "df_meta.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfMHyp_wCLIj"
   },
   "source": [
    "## Discussion\n",
    "\n",
    "\n",
    "\n",
    "1.   We can not read some categories (especially those with large data, in this case, the sofrware category) directly in Spark; Need to figure out why and how to solve this. (Tried reading by in raw and use pandas and then spark, but this also failed.)\n",
    "(I skipped this question by choosing another category which is fine to read directly)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lporcTWBqQq"
   },
   "source": [
    "# Data Pre-Processing and visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgWrDtZ94w89"
   },
   "outputs": [],
   "source": [
    "# Drop the image column because I don't think we need it\n",
    "df = df.drop('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoBwuSpS0a_A"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, length, unix_timestamp, to_date\n",
    "\n",
    "# Handling missing values: For simplicity, we'll drop rows with any NULLs\n",
    "df = df.na.drop()\n",
    "\n",
    "# Filtering out unverified reviews\n",
    "df = df.filter(col('verified') == True)\n",
    "\n",
    "# Feature Engineering - creating a new feature for the length of the reviewText\n",
    "df = df.withColumn('reviewText_length', length(col('reviewText')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7K4J7Ww92DLo",
    "outputId": "8de2942b-f31a-4cce-ba3f-34e8fe12dace"
   },
   "outputs": [],
   "source": [
    "# Show the processed DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "AH_C-uRAu5G-",
    "outputId": "90a23133-a2fc-4bb8-83ae-9c0d25fb384a"
   },
   "outputs": [],
   "source": [
    "\n",
    "review_lengths = df.select('reviewText_length').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Now convert this list to a Pandas Series\n",
    "pd_series = pd.Series(review_lengths)\n",
    "\n",
    "# Plotting the distribution of reviewText_length using Pandas/Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "pd_series.hist(bins=50)  # Adjust the number of bins for your specific dataset\n",
    "plt.title('Distribution of Review Text Lengths')\n",
    "plt.xlabel('Review Text Length')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSoJRRFQ2ApP"
   },
   "outputs": [],
   "source": [
    "# Example of filtering based on a condition, such as reviews that are too short/long might be outliers\n",
    "df = df.filter(col('reviewText_length') > 10)  # Example threshold\n",
    "\n",
    "df = df.filter(col('reviewText_length') < 1000)  # Example threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yIjISgc1xjx"
   },
   "outputs": [],
   "source": [
    "# Text Processing - cleaning the review text, tokenizing, and removing stop words (simplified example)\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "tokenizer = Tokenizer(inputCol='reviewText', outputCol='reviewText_tokens')\n",
    "df = tokenizer.transform(df)\n",
    "remover = StopWordsRemover(inputCol='reviewText_tokens', outputCol='reviewText_clean')\n",
    "df = remover.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRKiCOYH1Ysa",
    "outputId": "b4b39f17-f458-4347-ca52-1274b04536bc"
   },
   "outputs": [],
   "source": [
    "# Show the processed DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqZcdGp-5cui"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, length, split\n",
    "\n",
    "# Split reviewTime into day, month, and year\n",
    "split_col = split(df['reviewTime'], ' ')\n",
    "df = df.withColumn('Day', split_col.getItem(0))\n",
    "df = df.withColumn('Month', split_col.getItem(1).substr(0,2))\n",
    "df = df.withColumn('Year', split_col.getItem(2))\n",
    "df = df.drop('reviewTime')\n",
    "\n",
    "# Convert 'vote' to integer type\n",
    "df = df.withColumn('vote', df['vote'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jl6pleTn6xRy",
    "outputId": "1de7cf9e-9764-4d33-9e85-f9f65a67d69e"
   },
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCawwLrJ2wv5",
    "outputId": "ab573a2d-74b7-415a-8060-020d234c4853"
   },
   "outputs": [],
   "source": [
    "df.select('reviewerName').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdBoW7v721fO",
    "outputId": "92fedbf3-4f68-40c7-9d62-938b99e9bedf"
   },
   "outputs": [],
   "source": [
    "df.select('asin').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDu-xVsaDMh-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "# Average rating\n",
    "average_rating = df.agg(avg(\"overall\")).first()[0]\n",
    "\n",
    "# Count of reviews for each asin\n",
    "reviews_per_asin = df.groupBy(\"asin\").agg(count(\"reviewerID\").alias(\"Number_of_Reviews\"))\n",
    "\n",
    "# Summary for vote column\n",
    "votes_summary = df.describe(['vote'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oseS27cytWcg"
   },
   "outputs": [],
   "source": [
    "# Count of reviews for each reviewer\n",
    "reviews_per_reviewers = df.groupBy(\"reviewerID\").agg(count(\"asin\").alias(\"Number_of_Reviews\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CceniMOxrS8o",
    "outputId": "254d1cb2-b3f9-41d7-c037-eab768341dac"
   },
   "outputs": [],
   "source": [
    "votes_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6OHclZxHDPTE",
    "outputId": "30222ae8-4009-4309-80bb-1deeae05f1c5"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert Spark DataFrame to Pandas for Visualization\n",
    "pdf = df.toPandas()\n",
    "reviews_per_asin_pdf = reviews_per_asin.toPandas()\n",
    "\n",
    "# Histogram for overall ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "pdf['overall'].hist()\n",
    "plt.title('Distribution of Overall Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the pie chart for overall ratings\n",
    "rating_counts = pdf['overall'].value_counts().sort_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Overall Ratings')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "MGZpqoW9s-68",
    "outputId": "78848733-cf9f-4212-b896-e66bc0f68c9e"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Bar chart for number of reviews for each asin\n",
    "top_reviews_per_asin_pdf = reviews_per_asin_pdf.sort_values(by='Number_of_Reviews', ascending=False).head(100)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Set the 'asin' as the index and plot the 'review_count' column\n",
    "top_reviews_per_asin_pdf.set_index('asin')['Number_of_Reviews'].plot(kind='bar', legend=False)\n",
    "plt.title('Number of Reviews for Top 100 ASINs')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('ASIN')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "ZY61YbgAtiE8",
    "outputId": "b785ec0d-372d-42a2-e900-5f7d0a93b2af"
   },
   "outputs": [],
   "source": [
    "# Bar chart for number of reviews for each reviewers\n",
    "reviews_per_reviewers_pdf = reviews_per_reviewers.toPandas()\n",
    "\n",
    "top_reviews_per_reviewers_pdf = reviews_per_reviewers_pdf.sort_values(by='Number_of_Reviews', ascending=False).head(100)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Set the 'reviewers' as the index and plot the 'review_count' column\n",
    "top_reviews_per_reviewers_pdf.set_index('reviewerID')['Number_of_Reviews'].plot(kind='bar', legend=False)\n",
    "plt.title('Number of Reviews for Top 100 reviewers')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('reviewers')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_QVtyi4Ewab"
   },
   "outputs": [],
   "source": [
    "# Convert reviewText and summary columns to a single string\n",
    "review_text_str = ' '.join(df.rdd.map(lambda row: row.reviewText).collect())\n",
    "summary_str = ' '.join(df.rdd.map(lambda row: row.summary).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871
    },
    "id": "0UBh50DFFI1l",
    "outputId": "f5c3b337-fa19-423e-cc2f-b8dffafb8c09"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_word_cloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "generate_word_cloud(review_text_str, \"Word Cloud for Review Text\")\n",
    "generate_word_cloud(summary_str, \"Word Cloud for Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az7mi7WwkR5f"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIVXnBAuuVf5"
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnzc-mTpuYuj"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Indexing is required to convert string identifiers to numeric indices for the ALS algorithm\n",
    "indexer_user = StringIndexer(inputCol=\"reviewerID\", outputCol=\"userIndex\")\n",
    "df = indexer_user.fit(df).transform(df)\n",
    "\n",
    "indexer_item = StringIndexer(inputCol=\"asin\", outputCol=\"itemIndex\")\n",
    "df = indexer_item.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnzc-mTpuYuj"
   },
   "outputs": [],
   "source": [
    "(train, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csuWnWesuTGB"
   },
   "source": [
    "\n",
    "## ALS\n",
    "\n",
    "refer: https://www.kaggle.com/code/nadianizam/h-m-fashion-recommendation-with-pyspark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVGz5N03kT7T"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Setting up the ALS model\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userIndex\", itemCol=\"itemIndex\", ratingCol=\"overall\", coldStartStrategy=\"drop\")\n",
    "\n",
    "# Fitting the ALS model on the training data\n",
    "model = als.fit(train)\n",
    "\n",
    "# Predicting on the train and test sets\n",
    "predictions_train = model.transform(train)\n",
    "predictions_test = model.transform(test)\n",
    "\n",
    "# Create an RMSE evaluator using the label and predicted columns\n",
    "reg_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"overall\", predictionCol=\"prediction\")\n",
    "\n",
    "# Evaluate the model on training data\n",
    "rmse_train = reg_evaluator.evaluate(predictions_train)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "rmse_test = reg_evaluator.evaluate(predictions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.recommendForAllUsers(10).show(1, truncate = False)\n",
    "model.save(\"models/RatingALSModel.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SIkh4pJbkT-_",
    "outputId": "89a85da8-d6a6-42b0-ef0d-56e42861934e"
   },
   "outputs": [],
   "source": [
    "rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4t13CqrkUCj",
    "outputId": "2443eee2-ea8b-4fc4-bb72-18ff59e75711"
   },
   "outputs": [],
   "source": [
    "rmse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALSModel\n",
    "recommendnum = 10\n",
    "als_model = ALSModel.load(\"models/RatingALSModel.obj\")\n",
    "als_model.recommendForAllUsers(recommendnum).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.groupBy(\"userIndex\").count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.filter(train['userIndex'] == testUserID).select('itemIndex').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.select('userIndex').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_temp = als_model.recommendForAllUsers(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = df_temp.select('recommendations').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,explode\n",
    "df_temp0 = df_temp.withColumn(\"name\", explode(col('recommendations')))\n",
    "df_temp0.withColumn(\"name2\", col('name').getItem(0)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,expr\n",
    "df_temp0 = df_temp.withColumn(\"name\", expr(\"transform(recommendations, r -> r.itemIndex)\"))\n",
    "df_temp0.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.filter(train['userIndex'] == m.userIndex).select('itemIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import collect_list\n",
    "actualitem = train.groupby('userIndex').agg(collect_list('itemIndex').alias('indexlist'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = df_temp0.join(actualitem, 'userIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array_intersect, size, avg\n",
    "metrics = result.withColumn('TP', size(array_intersect(col('name'),col('indexlist')))) \\\n",
    "                .withColumn('precision', col('TP')/recommendnum) \\\n",
    "                .withColumn('recall', col('TP')/size(col('indexlist'))) \\\n",
    "                .withColumn('F1', when((col('precision') + col('recall')) >0,\n",
    "                                        2* (col('precision') *col('recall'))/(col('precision') +col('recall')))\n",
    "                             .otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max as ma\n",
    "average_metrics = metrics.agg(\n",
    "    ma(col('precision')),\n",
    "    ma(col('recall')),\n",
    "    ma(col('F1'))\n",
    ")\n",
    "average_metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp0.withColumn(\"name2\", col('name').getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in als_model.userFactors.select(\"id\").collect():\n",
    "    userId = r.id\n",
    "    cateId_df = pd.DataFrame(pdf.cateId,unique(),columns=['cateId'])\n",
    "    cateId_df.insert(0,'userId',np.array([userId for i in range(6769)]))\n",
    "\n",
    "    ret = set()\n",
    "    # 利用模型，传入datasets(userId, cateId)，这里控制了userId一样，所以相当于是在求某用户对所有分类的兴趣程度\n",
    "    cateId_list = als_model.transform(spark.createDataFrame(cateId_df)).sort('prediction',ascending=False).na.drop()\n",
    "   \n",
    "    # 从前20个分类中选出500个进行召回\n",
    "    for i in cateId_list.head(20):\n",
    "        need = 500 - len(ret)    # 如果不足500个，那么随机选出need个广告\n",
    "        ret = ret.union(np.random.choice(pdf.where(pdf.cateId==i.cateId).adgroupId.dropna().astype(np.int64),need))\n",
    "        if len(ret) >= 500:    # 如果达到500个则退出\n",
    "            break\n",
    "    client.sadd(userId, *ret)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeLO7apex1pt"
   },
   "source": [
    "## GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrR_GanHyVEe",
    "outputId": "5d8a705f-e9c2-417b-f96f-9819e55a8c76"
   },
   "outputs": [],
   "source": [
    "!pip install graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uEIXzWg_wvOG",
    "outputId": "a924f54d-81f5-4b0c-acb4-03769b48bc40"
   },
   "outputs": [],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "# Assuming 'df' is your original PySpark DataFrame and it has been indexed\n",
    "# Create vertices DataFrame\n",
    "vertices = df.selectExpr(\"userIndex as id\").distinct().union(df.selectExpr(\"itemIndex as id\").distinct()) # check if those two index overlap\n",
    "\n",
    "# Create edges DataFrame\n",
    "edges = df.selectExpr(\"userIndex as src\", \"itemIndex as dst\", \"overall as rating\")\n",
    "\n",
    "# Create a GraphFrame\n",
    "graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzjaV0l0yTCw"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# 1: Hyperparameter tuning\n",
    "# 2: How to make use of the graph functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5euM0Dd21S6-"
   },
   "source": [
    "# Meta Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"item总数：\", df_meta.groupBy(\"asin\").count().count())\n",
    "print(\"brand总数：\", df_meta.groupBy(\"brand\").count().count())\n",
    "print(\"category总数：\", df_meta.groupBy(\"category\").count().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"分类特征值个数情况: \")\n",
    "print(\"cms_segid: \", user_profile_df.groupBy(\"cms_segid\").count().count())\n",
    "print(\"cms_group_id: \", user_profile_df.groupBy(\"cms_group_id\").count().count())\n",
    "print(\"final_gender_code: \", user_profile_df.groupBy(\"final_gender_code\").count().count())\n",
    "print(\"age_level: \", user_profile_df.groupBy(\"age_level\").count().count())\n",
    "print(\"shopping_level: \", user_profile_df.groupBy(\"shopping_level\").count().count())\n",
    "print(\"occupation: \", user_profile_df.groupBy(\"occupation\").count().count())\n",
    "\n",
    "print(\"含缺失值的特征情况: \")\n",
    "df_meta.groupBy(\"details\").count().show()\n",
    "df_meta.groupBy(\"feature\").count().show()\n",
    "df_meta.groupBy(\"fit\").count().show()\n",
    "\n",
    "t_count = df_meta.count()\n",
    "\n",
    "pl_na_count = t_count - user_profile_df.dropna(subset=[\"pvalue_level\"]).count()\n",
    "print(\"pvalue_level的空值情况：\", pl_na_count, \"空值占比：%0.2f%%\"%(pl_na_count/t_count*100))\n",
    "\n",
    "nul_na_count = t_count - user_profile_df.dropna(subset=[\"new_user_class_level\"]).count()\n",
    "print(\"new_user_class_level的空值情况：\", nul_na_count, \"空值占比：%0.2f%%\"%(nul_na_count/t_count*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"含缺失值的特征情况: \")\n",
    "df_meta.groupBy(\"details\").count().show()\n",
    "df_meta.groupBy(\"feature\").count().show()\n",
    "df_meta.groupBy(\"fit\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DS5110 Spark 3.3",
   "language": "python",
   "name": "ds5110_spark3.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
