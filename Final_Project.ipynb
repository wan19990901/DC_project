{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "f5NDXrN2CtH7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIpCl4lKHY8D",
    "outputId": "b1e549d6-cb93-44f1-f65f-a10c6434c2cd"
   },
   "outputs": [],
   "source": [
    "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/categoryFiles/Industrial_and_Scientific.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb5fGi2wJpXI",
    "outputId": "9361b9de-dedc-4032-f99a-80e68b110106"
   },
   "outputs": [],
   "source": [
    "!wget https://datarepo.eng.ucsd.edu/mcauley_group/data/amazon_v2/metaFiles2/meta_Industrial_and_Scientific.json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBfzThdejDSJ",
    "outputId": "91a779c0-1601-49f8-9d1d-c711117a776e"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddVjvBQ5BnXq"
   },
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "G8Qexuq_3Rx0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 00:03:01 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"readGZ\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.default.parallelism\", 24) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 24) \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the gzipped JSON file directly into a DataFrame\n",
    "df = spark.read.json(\"Industrial_and_Scientific.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSkWP10AtaKv",
    "outputId": "219a8d98-5f89-41f8-eb8e-3b053f11ca9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+--------------------+-----------+--------------+----------------+-----+--------------------+--------------+--------+----+\n",
      "|      asin|image|overall|          reviewText| reviewTime|    reviewerID|    reviewerName|style|             summary|unixReviewTime|verified|vote|\n",
      "+----------+-----+-------+--------------------+-----------+--------------+----------------+-----+--------------------+--------------+--------+----+\n",
      "|0176496920| null|    5.0|Arrived on time, ...|01 23, 2013|A3FANY5GOT5X0W|    Kelly Keyser| null|  Just as described!|    1358899200|    true|null|\n",
      "|0176496920| null|    5.0|This device was h...| 11 5, 2012| AT6HRPPYOPHMB|       Michael C| null|        Great device|    1352073600|    true|null|\n",
      "|0176496920| null|    4.0|Just a clicker no...|10 17, 2012| A4IX7B38LIN1E|              BH| null|         Pretty Good|    1350432000|    true|null|\n",
      "|0176496920| null|    5.0|Great response ca...|03 29, 2017|A12Q4LR8N17AOZ|   Waterfall3500| null|Thank you for the...|    1490745600|    true|null|\n",
      "|0176496920| null|    1.0|It only lasted fo...|03 21, 2017|A1GJXZZPOZ3OD9| Amazon Customer| null|            One Star|    1490054400|    true|null|\n",
      "|0176496920| null|    1.0|There is no licen...|02 22, 2017| AAHQWAS1NA6AX|Kimberly Alvarez| null|             License|    1487721600|    true|null|\n",
      "|0176496920| null|    5.0|Arrived perfectly...|02 14, 2017|A1IFO0SYT4VYHQ| Amazon Customer| null|     Would recommend|    1487030400|    true|null|\n",
      "|0176496920| null|    5.0|        Works great!|02 13, 2017|A2EYTYCKW20APK| Amazon Customer| null|          Five Stars|    1486944000|    true|null|\n",
      "|0176496920| null|    5.0|I got this in les...|02 10, 2017|A3JSW99QMYGZNL|          Yadira| null|       In good shape|    1486684800|    true|null|\n",
      "|0176496920| null|    3.0|Came in quickly a...|01 24, 2017| A71I4604KUANI|  Kirsten Draper| null|Came in quick but...|    1485216000|    true|null|\n",
      "+----------+-----+-------+--------------------+-----------+--------------+----------------+-----+--------------------+--------------+--------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame to check if it's loaded correctly\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "7igYuRaV4bF7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### load the meta data\n",
    "\n",
    "df_meta = spark.read.json(\"meta_Industrial_and_Scientific.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vv5iGqYNiq9L",
    "outputId": "39501445-92da-4e37-9eec-3dee6c081402"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+--------------------+--------------------+-----------------+--------------------+-------+--------------------+---+--------------------+--------------------+--------------------+------+--------------------+------------+--------------------+-----+--------------------+\n",
      "|            also_buy|           also_view|      asin|               brand|            category|             date|         description|details|             feature|fit|            imageURL|     imageURLHighRes|            main_cat| price|                rank|similar_item|               tech1|tech2|               title|\n",
      "+--------------------+--------------------+----------+--------------------+--------------------+-----------------+--------------------+-------+--------------------+---+--------------------+--------------------+--------------------+------+--------------------+------------+--------------------+-----+--------------------+\n",
      "|[1454894547, 0133...|                  []|0176496920|Turning Technolog...|[Industrial & Sci...|                 |[RCRF-03 - Works ...|   null|                  []|   |[https://images-n...|[https://images-n...|Industrial & Scie...|$23.61|12,329 in Industr...|            |                    |     |Turning Technolog...|\n",
      "|[B01NBCNTJ9, B01I...|                  []|0692782109|               R-Cat|                  []|                 |[The only laminat...|   null|                  []|   |[https://images-n...|[https://images-n...|Industrial & Scie...|$20.00|20,174 in Industr...|            |                    |     |R-Cat 692782109 E...|\n",
      "|[0781776821, 0781...|                  []|0781776848|Anatomical Chart ...|[Industrial & Sci...|   August 7, 2007|[<div>, Now in it...|   null|[MPN: 97807817768...|   |                  []|                  []|     Office Products|$10.37|[\">#351,522 in Of...|            | class=\"a-keyvalu...|     |Anatomical Chart ...|\n",
      "|[0781786630, 1587...|[B07BCNHC3K, B07D...|0781786606|Anatomical Chart ...|[Industrial & Sci...|February 26, 2009|[<div>, Developed...|   null|[MPN: 97807817866...|   |                  []|                  []|     Office Products|      |[\">#459,493 in Of...|            | class=\"a-keyvalu...|     |Joints of the Low...|\n",
      "|[0077402286, 1455...|                  []|0840026080|Responsive Innova...|[Industrial & Sci...|                 |[Advanced digital...|   null|                  []|   |[https://images-n...|[https://images-n...|Industrial & Scie...|$32.00|172,566 in Indust...|            |                    |     |Turning Technolog...|\n",
      "|[089455767X, 0894...|                  []|0894558358|   Critical Thinking|                  []|                 |[The mind-buildin...|   null|                  []|   |[https://images-n...|[https://images-n...|Industrial & Scie...|$21.99|91,253 in Industr...|            |                    |     |Science Detective...|\n",
      "|        [1572224975]|[0760724083, B078...|0971007004|    Thomas McCracken|[Industrial & Sci...|                 |                  []|   null|                  []|   |                  []|                  []|               Books|$13.32|1,104,587 in Books (|            |                    |     |Wall Chart of Hum...|\n",
      "|[1587790211, 1587...|                  []|1587790319|Anatomical Chart ...|[Industrial & Sci...|September 6, 2006|[<div>&#160;The <...|   null|[Provides anterio...|   |                  []|                  []|     Office Products|$14.82|[\">#90,545 in Off...|            | class=\"a-keyvalu...|     |The Male Reproduc...|\n",
      "|[B004MAJHLW, 1587...|                  []|1587792052|             Unknown|[Industrial & Sci...|                 |[Illustrates spin...|   null|                  []|   |                  []|                  []|Industrial & Scie...|$23.84|17,125 in Industr...|            |                    |     |The Spinal Nerves...|\n",
      "|[1587798085, 1587...|                  []|1587791420|  Physician Supplies|[Industrial & Sci...|   August 4, 2005|[<DIV>Shows muscl...|   null|[Shows muscle and...|   |                  []|                  []|     Office Products|$24.45|[\">#318,099 in Of...|            | class=\"a-keyvalu...|     |Hand and Wrist An...|\n",
      "+--------------------+--------------------+----------+--------------------+--------------------+-----------------+--------------------+-------+--------------------+---+--------------------+--------------------+--------------------+------+--------------------+------------+--------------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_meta.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfMHyp_wCLIj"
   },
   "source": [
    "## Discussion\n",
    "\n",
    "\n",
    "\n",
    "1.   We can not read some categories (especially those with large data, in this case, the sofrware category) directly in Spark; Need to figure out why and how to solve this. (Tried reading by in raw and use pandas and then spark, but this also failed.)\n",
    "(I skipped this question by choosing another category which is fine to read directly)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lporcTWBqQq"
   },
   "source": [
    "# Data Pre-Processing and visulization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LgWrDtZ94w89"
   },
   "outputs": [],
   "source": [
    "# Drop the image column because I don't think we need it\n",
    "df = df.drop('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HoBwuSpS0a_A"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, length, unix_timestamp, to_date\n",
    "\n",
    "# Handling missing values: For simplicity, we'll drop rows with any NULLs\n",
    "df = df.na.drop()\n",
    "\n",
    "# Filtering out unverified reviews\n",
    "df = df.filter(col('verified') == True)\n",
    "\n",
    "# Feature Engineering - creating a new feature for the length of the reviewText\n",
    "df = df.withColumn('reviewText_length', length(col('reviewText')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7K4J7Ww92DLo",
    "outputId": "7810957f-2851-4b36-9834-31bb22726bb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+-----------+--------------+-------------+--------------------+--------------------+--------------+--------+----+-----------------+\n",
      "|      asin|overall|          reviewText| reviewTime|    reviewerID| reviewerName|               style|             summary|unixReviewTime|verified|vote|reviewText_length|\n",
      "+----------+-------+--------------------+-----------+--------------+-------------+--------------------+--------------------+--------------+--------+----+-----------------+\n",
      "|1587790637|    5.0|I love everything...| 08 3, 2014| ALORN8T3UJC88| Ji Sung Moon|{null, null, null...|Great studying ma...|    1407024000|    true|   6|              390|\n",
      "|1587790637|    2.0|Disappointed with...| 06 9, 2014|A2DR04A9BINUE6|        Becky|{null, null, null...|             Creased|    1402272000|    true|   4|              157|\n",
      "|1587790637|    1.0|Very poor packagi...| 04 8, 2014|A39NP41GNG6DJT|  Nicole Ryan|{null, null, null...|Poster bent and w...|    1396915200|    true|   8|              286|\n",
      "|1587790637|    5.0|Clear font, easy ...| 04 1, 2017|A22L35OKSZBXE6|  Outdoorsy29|{null, null, null...|        easy to read|    1491004800|    true|   2|               81|\n",
      "|1587790637|    5.0|Great image quali...|10 15, 2016| A8QBLKY1N72GG|LovelyNutty31|{null, null, null...|           Recommend|    1476489600|    true|   3|              217|\n",
      "|1587790637|    5.0|This poster is aw...|03 27, 2016|A1NBP94KU536L6|  Esther Hong|{null, null, null...|Perfect for med/n...|    1459036800|    true|   2|              289|\n",
      "|1587790637|    2.0|The product itsel...|09 30, 2015|A32AJ6YUG1FCQY|         Mary|{null, null, null...|The product itsel...|    1443571200|    true|   2|              179|\n",
      "|1587790637|    2.0|Came in poor cond...|03 23, 2015| AHX9VS31SMYWN|    Ronnie Yu|{null, null, null...|           Two Stars|    1427068800|    true|   4|              103|\n",
      "|B0000223SK|    5.0|When working with...|01 15, 2012| AKPWF09K5F5OZ|Marge Wellman|{null, null, null...|       Lasts & Lasts|    1326585600|    true|   2|              128|\n",
      "|B0000223SK|    5.0|Lasts for several...|12 14, 2011|A29ATM05PXSQY7|      Mr Phil|{null, null, null...|   Serious sandpaper|    1323820800|    true|   5|              246|\n",
      "+----------+-------+--------------------+-----------+--------------+-------------+--------------------+--------------------+--------------+--------+----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the processed DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "AH_C-uRAu5G-",
    "outputId": "32bb7fa5-58e4-4940-e1cc-aca61a0789c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "review_lengths = df.select('reviewText_length').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Now convert this list to a Pandas Series\n",
    "pd_series = pd.Series(review_lengths)\n",
    "\n",
    "# Plotting the distribution of reviewText_length using Pandas/Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "pd_series.hist(bins=50)  # Adjust the number of bins for your specific dataset\n",
    "plt.title('Distribution of Review Text Lengths')\n",
    "plt.xlabel('Review Text Length')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LSoJRRFQ2ApP"
   },
   "outputs": [],
   "source": [
    "# Example of filtering based on a condition, such as reviews that are too short/long might be outliers\n",
    "df = df.filter(col('reviewText_length') > 10)  # Example threshold\n",
    "\n",
    "df = df.filter(col('reviewText_length') < 1000)  # Example threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4yIjISgc1xjx"
   },
   "outputs": [],
   "source": [
    "# Text Processing - cleaning the review text, tokenizing, and removing stop words (simplified example)\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "tokenizer = Tokenizer(inputCol='reviewText', outputCol='reviewText_tokens')\n",
    "df = tokenizer.transform(df)\n",
    "remover = StopWordsRemover(inputCol='reviewText_tokens', outputCol='reviewText_clean')\n",
    "df = remover.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRKiCOYH1Ysa",
    "outputId": "089447bd-7007-4edf-cf2d-ba89c00c8af4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------------+-----------+--------------+-------------+--------------------+--------------------+--------------+--------+----+-----------------+--------------------+--------------------+\n",
      "|      asin|overall|          reviewText| reviewTime|    reviewerID| reviewerName|               style|             summary|unixReviewTime|verified|vote|reviewText_length|   reviewText_tokens|    reviewText_clean|\n",
      "+----------+-------+--------------------+-----------+--------------+-------------+--------------------+--------------------+--------------+--------+----+-----------------+--------------------+--------------------+\n",
      "|1587790637|    5.0|I love everything...| 08 3, 2014| ALORN8T3UJC88| Ji Sung Moon|{null, null, null...|Great studying ma...|    1407024000|    true|   6|              390|[i, love, everyth...|[love, everything...|\n",
      "|1587790637|    2.0|Disappointed with...| 06 9, 2014|A2DR04A9BINUE6|        Becky|{null, null, null...|             Creased|    1402272000|    true|   4|              157|[disappointed, wi...|[disappointed, pr...|\n",
      "|1587790637|    1.0|Very poor packagi...| 04 8, 2014|A39NP41GNG6DJT|  Nicole Ryan|{null, null, null...|Poster bent and w...|    1396915200|    true|   8|              286|[very, poor, pack...|[poor, packaging!...|\n",
      "|1587790637|    5.0|Clear font, easy ...| 04 1, 2017|A22L35OKSZBXE6|  Outdoorsy29|{null, null, null...|        easy to read|    1491004800|    true|   2|               81|[clear, font,, ea...|[clear, font,, ea...|\n",
      "|1587790637|    5.0|Great image quali...|10 15, 2016| A8QBLKY1N72GG|LovelyNutty31|{null, null, null...|           Recommend|    1476489600|    true|   3|              217|[great, image, qu...|[great, image, qu...|\n",
      "|1587790637|    5.0|This poster is aw...|03 27, 2016|A1NBP94KU536L6|  Esther Hong|{null, null, null...|Perfect for med/n...|    1459036800|    true|   2|              289|[this, poster, is...|[poster, awesome!...|\n",
      "|1587790637|    2.0|The product itsel...|09 30, 2015|A32AJ6YUG1FCQY|         Mary|{null, null, null...|The product itsel...|    1443571200|    true|   2|              179|[the, product, it...|[product, great.,...|\n",
      "|1587790637|    2.0|Came in poor cond...|03 23, 2015| AHX9VS31SMYWN|    Ronnie Yu|{null, null, null...|           Two Stars|    1427068800|    true|   4|              103|[came, in, poor, ...|[came, poor, cond...|\n",
      "|B0000223SK|    5.0|When working with...|01 15, 2012| AKPWF09K5F5OZ|Marge Wellman|{null, null, null...|       Lasts & Lasts|    1326585600|    true|   2|              128|[when, working, w...|[working, hard, w...|\n",
      "|B0000223SK|    5.0|Lasts for several...|12 14, 2011|A29ATM05PXSQY7|      Mr Phil|{null, null, null...|   Serious sandpaper|    1323820800|    true|   5|              246|[lasts, for, seve...|[lasts, several, ...|\n",
      "+----------+-------+--------------------+-----------+--------------+-------------+--------------------+--------------------+--------------+--------+----+-----------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the processed DataFrame\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WqZcdGp-5cui"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, length, split\n",
    "\n",
    "# Split reviewTime into day, month, and year\n",
    "split_col = split(df['reviewTime'], ' ')\n",
    "df = df.withColumn('Day', split_col.getItem(0))\n",
    "df = df.withColumn('Month', split_col.getItem(1).substr(0,2))\n",
    "df = df.withColumn('Year', split_col.getItem(2))\n",
    "df = df.drop('reviewTime')\n",
    "\n",
    "# Convert 'vote' to integer type\n",
    "df = df.withColumn('vote', df['vote'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jl6pleTn6xRy",
    "outputId": "66f4499d-d4f6-419a-ea62-39b8cd8ef432"
   },
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LCawwLrJ2wv5",
    "outputId": "20b71eeb-3473-4b41-83e3-731fa820acf6"
   },
   "outputs": [],
   "source": [
    "df.select('reviewerName').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VdBoW7v721fO",
    "outputId": "a749bec3-f3f1-4a45-e62a-fd6600c8cb07"
   },
   "outputs": [],
   "source": [
    "df.select('asin').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDu-xVsaDMh-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "# Average rating\n",
    "average_rating = df.agg(avg(\"overall\")).first()[0]\n",
    "\n",
    "# Count of reviews for each asin\n",
    "reviews_per_asin = df.groupBy(\"asin\").agg(count(\"reviewerID\").alias(\"Number_of_Reviews\"))\n",
    "\n",
    "# Summary for vote column\n",
    "votes_summary = df.describe(['vote'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oseS27cytWcg"
   },
   "outputs": [],
   "source": [
    "# Count of reviews for each reviewer\n",
    "reviews_per_reviewers = df.groupBy(\"reviewerID\").agg(count(\"asin\").alias(\"Number_of_Reviews\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CceniMOxrS8o",
    "outputId": "4ede7154-8caf-403d-f2dc-c7e58b217ca6"
   },
   "outputs": [],
   "source": [
    "votes_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6OHclZxHDPTE",
    "outputId": "ede6e493-3c8d-4edb-82fd-46e8c22322fd"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for Visualization\n",
    "pdf = df.toPandas()\n",
    "reviews_per_asin_pdf = reviews_per_asin.toPandas()\n",
    "\n",
    "# Histogram for overall ratings\n",
    "plt.figure(figsize=(10, 6))\n",
    "pdf['overall'].hist()\n",
    "plt.title('Distribution of Overall Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the pie chart for overall ratings\n",
    "rating_counts = pdf['overall'].value_counts().sort_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Distribution of Overall Ratings')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "MGZpqoW9s-68",
    "outputId": "9f685d49-2ec7-4814-c40e-cea8ee0499ea"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Bar chart for number of reviews for each asin\n",
    "top_reviews_per_asin_pdf = reviews_per_asin_pdf.sort_values(by='Number_of_Reviews', ascending=False).head(100)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Set the 'asin' as the index and plot the 'review_count' column\n",
    "top_reviews_per_asin_pdf.set_index('asin')['Number_of_Reviews'].plot(kind='bar', legend=False)\n",
    "plt.title('Number of Reviews for Top 100 ASINs')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('ASIN')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "ZY61YbgAtiE8",
    "outputId": "00d7f9fc-b274-4a6e-b188-b563d6de82a2"
   },
   "outputs": [],
   "source": [
    "# Bar chart for number of reviews for each reviewers\n",
    "reviews_per_reviewers_pdf = reviews_per_reviewers.toPandas()\n",
    "\n",
    "top_reviews_per_reviewers_pdf = reviews_per_reviewers_pdf.sort_values(by='Number_of_Reviews', ascending=False).head(100)\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Set the 'reviewers' as the index and plot the 'review_count' column\n",
    "top_reviews_per_reviewers_pdf.set_index('reviewerID')['Number_of_Reviews'].plot(kind='bar', legend=False)\n",
    "plt.title('Number of Reviews for Top 100 reviewers')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('reviewers')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_QVtyi4Ewab"
   },
   "outputs": [],
   "source": [
    "# Convert reviewText and summary columns to a single string\n",
    "review_text_str = ' '.join(df.rdd.map(lambda row: row.reviewText).collect())\n",
    "summary_str = ' '.join(df.rdd.map(lambda row: row.summary).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 871
    },
    "id": "0UBh50DFFI1l",
    "outputId": "b787afb4-42cb-4d8c-d022-ca1f1be3a438"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_word_cloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "generate_word_cloud(review_text_str, \"Word Cloud for Review Text\")\n",
    "generate_word_cloud(summary_str, \"Word Cloud for Summary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az7mi7WwkR5f"
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIVXnBAuuVf5"
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mnzc-mTpuYuj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Indexing is required to convert string identifiers to numeric indices for the ALS algorithm\n",
    "indexer_user = StringIndexer(inputCol=\"reviewerID\", outputCol=\"userIndex\")\n",
    "df = indexer_user.fit(df).transform(df)\n",
    "\n",
    "indexer_item = StringIndexer(inputCol=\"asin\", outputCol=\"itemIndex\")\n",
    "df = indexer_item.fit(df).transform(df)\n",
    "\n",
    "(train, test) = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csuWnWesuTGB"
   },
   "source": [
    "\n",
    "## ALS\n",
    "\n",
    "refer: https://www.kaggle.com/code/nadianizam/h-m-fashion-recommendation-with-pyspark\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVGz5N03kT7T"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Setting up the ALS model\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userIndex\", itemCol=\"itemIndex\", ratingCol=\"overall\", coldStartStrategy=\"drop\")\n",
    "\n",
    "# Fitting the ALS model on the training data\n",
    "model = als.fit(train)\n",
    "\n",
    "# Predicting on the train and test sets\n",
    "predictions_train = model.transform(train)\n",
    "predictions_test = model.transform(test)\n",
    "\n",
    "# Create an RMSE evaluator using the label and predicted columns\n",
    "reg_evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"overall\", predictionCol=\"prediction\")\n",
    "\n",
    "# Evaluate the model on training data\n",
    "rmse_train = reg_evaluator.evaluate(predictions_train)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "rmse_test = reg_evaluator.evaluate(predictions_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SIkh4pJbkT-_",
    "outputId": "c434f099-f5cd-4655-9d3f-7c34069be035"
   },
   "outputs": [],
   "source": [
    "rmse_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h4t13CqrkUCj",
    "outputId": "fab8aa57-3366-45cb-ae43-9b67488e6db2"
   },
   "outputs": [],
   "source": [
    "rmse_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeLO7apex1pt"
   },
   "source": [
    "## GraphFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrR_GanHyVEe",
    "outputId": "27552fb0-c701-4a17-e97d-2d783b1549fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting graphframes\n",
      "  Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: numpy in /apps/software/standard/core/pyspark/3.4.1-py3.11/lib/python3.11/site-packages (from graphframes) (1.25.2)\n",
      "Collecting nose (from graphframes)\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nose, graphframes\n",
      "\u001b[33m  WARNING: The scripts nosetests and nosetests-3.4 are installed in '/home/wxr9et/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed graphframes-0.6 nose-1.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install graphframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "id": "uEIXzWg_wvOG",
    "outputId": "c8615bb2-5d47-4da5-a829-a4fccd18e2d4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphframes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgraphframes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphFrame\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Assuming 'df' is your original PySpark DataFrame and it has been indexed\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create vertices DataFrame\u001b[39;00m\n\u001b[1;32m      5\u001b[0m vertices \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muserIndex as id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()\u001b[38;5;241m.\u001b[39munion(df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitemIndex as id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdistinct()) \u001b[38;5;66;03m# check if those two index overlap\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphframes'"
     ]
    }
   ],
   "source": [
    "from graphframes import GraphFrame\n",
    "\n",
    "# Assuming 'df' is your original PySpark DataFrame and it has been indexed\n",
    "# Create vertices DataFrame\n",
    "vertices = df.selectExpr(\"userIndex as id\").distinct().union(df.selectExpr(\"itemIndex as id\").distinct()) # check if those two index overlap\n",
    "\n",
    "# Create edges DataFrame\n",
    "edges = df.selectExpr(\"userIndex as src\", \"itemIndex as dst\", \"overall as rating\")\n",
    "\n",
    "# Create a GraphFrame\n",
    "graph = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzjaV0l0yTCw"
   },
   "outputs": [],
   "source": [
    "# TO DO\n",
    "# 1: Hyperparameter tuning\n",
    "# 2: How to make use of the graph functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5euM0Dd21S6-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDhfmn8VUYCf"
   },
   "source": [
    "# Content-based Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "J-O-tSCQMNFA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_meta = spark.read.json(\"meta_Industrial_and_Scientific.json.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5XhsAxVMUFj",
    "outputId": "6d40483d-4524-433f-bbc3-54a3e211825a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               price|count|\n",
      "+--------------------+-----+\n",
      "|                    |42683|\n",
      "|.a-box-inner{back...| 1064|\n",
      "|               $9.99|  953|\n",
      "|               $7.99|  793|\n",
      "|               $8.99|  666|\n",
      "|               $6.99|  650|\n",
      "|              $14.99|  647|\n",
      "|              $19.99|  639|\n",
      "|              $12.99|  621|\n",
      "|              $15.99|  518|\n",
      "|              $11.99|  516|\n",
      "|               $5.99|  515|\n",
      "|               $1.41|  498|\n",
      "|              $13.99|  482|\n",
      "|               $9.95|  478|\n",
      "|              $10.99|  467|\n",
      "|               $4.99|  467|\n",
      "|              $29.99|  449|\n",
      "|              $16.99|  399|\n",
      "|              $12.95|  365|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_meta.groupBy('price').count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IniBwBFKMcib",
    "outputId": "99fef974-a062-4be4-e5c7-2f7d7e9c1ea3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               brand|count|\n",
      "+--------------------+-----+\n",
      "|              uxcell| 7147|\n",
      "|         Small Parts| 4252|\n",
      "|   The Hillman Group| 1868|\n",
      "|                    | 1302|\n",
      "|           SmartSign| 1164|\n",
      "|                  3M| 1021|\n",
      "|                 VXB| 1003|\n",
      "|               Brady|  866|\n",
      "|     ComplianceSigns|  793|\n",
      "|Hard-to-Find Fast...|  700|\n",
      "|                SEOH|  680|\n",
      "|   RAW PRODUCTS CORP|  660|\n",
      "|           Fastenere|  608|\n",
      "|             Generic|  582|\n",
      "|              Vestil|  573|\n",
      "|      D&D PowerDrive|  568|\n",
      "|             Medline|  560|\n",
      "|  Simpson Strong-Tie|  492|\n",
      "|               EISCO|  487|\n",
      "|             Hillman|  458|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_meta.groupBy('brand').count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "wh42k91Sg0rj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import mean\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: The item profiles are already in 'df_transformed' as 'features'\n",
    "\n",
    "from pyspark.sql.functions import avg, count, collect_list\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Replace null or empty strings with a placeholder\n",
    "df_meta = df_meta.na.fill({\"brand\": \"Unknown\"})\n",
    "df_meta = df_meta.withColumn(\"brand\", F.when(F.col(\"brand\") == \"\", \"Unknown\").otherwise(F.col(\"brand\")))\n",
    "\n",
    "# Step 1: Remove the dollar sign and try to convert the price to float\n",
    "df_meta = df_meta.withColumn(\"price\", regexp_replace(col(\"price\"), \"[\\$,]\", \"\").cast(\"float\"))\n",
    "\n",
    "# Step 2: Replace non-numeric (or negative) values with None, which Spark recognizes as null\n",
    "df_meta = df_meta.withColumn(\"price\", when(col(\"price\") < 0, None).otherwise(col(\"price\")))\n",
    "\n",
    "# Step 3: Calculate the average of the numeric prices\n",
    "average_price = df_meta.select(avg(col(\"price\")).alias(\"avg_price\")).collect()[0][\"avg_price\"]\n",
    "\n",
    "# Step 4: Fill null values with the average price\n",
    "df_meta = df_meta.na.fill({\"price\": average_price})\n",
    "\n",
    "\n",
    "# Ensure the 'brand' column exists and does not have empty strings or null values\n",
    "if 'brand' not in df_meta.columns:\n",
    "    raise ValueError(\"'brand' column is missing from the DataFrame.\")\n",
    "if df_meta.filter((col('brand') == \"\") | (col('brand').isNull())).count() > 0:\n",
    "    raise ValueError(\"'brand' column contains null or empty strings.\")\n",
    "\n",
    "# Ensure the 'price' column exists and has the correct data type\n",
    "if 'price' not in df_meta.columns:\n",
    "    raise ValueError(\"'price' column is missing from the DataFrame.\")\n",
    "if df_meta.filter(col('price').isNull()).count() > 0:\n",
    "    df_meta = df_meta.fillna({'price': 0})  # Replace nulls with 0 in 'price' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4gAPnNgOnLf",
    "outputId": "df7606a3-8e72-4768-bb5c-e3dcf191d384"
   },
   "outputs": [],
   "source": [
    "df_meta.select('brand').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9mKZCbZxOudr",
    "outputId": "f5859a34-6592-4a6e-8920-f093635c0f79"
   },
   "outputs": [],
   "source": [
    "df_meta.groupBy('brand').count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'train' and 'test' are the DataFrames resulting from splitting 'df',\n",
    "# and 'df_meta' is the DataFrame containing the metadata\n",
    "\n",
    "# Select the 'asin' from train and test DataFrames\n",
    "train_asin = train.select('asin').distinct()\n",
    "test_asin = test.select('asin').distinct()\n",
    "\n",
    "# Join the metadata with the asin DataFrames to filter the rows\n",
    "df_train = df_meta.join(train_asin, on='asin', how='inner')\n",
    "df_test = df_meta.join(test_asin, on='asin', how='inner')\n",
    "\n",
    "# Now df_train_meta and df_test_meta contain metadata for asins in the train and test sets respectively\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBmk7HVGgAda",
    "outputId": "85647255-26b7-409d-ecf7-2cfd9a6d1df2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 00:27:21 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 00:27:32 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 00:27:33 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 00:27:33 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 00:27:44 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|      asin|            also_buy|           also_view|               brand|            category|              date|         description|             details|             feature|fit|            imageURL|     imageURLHighRes|            main_cat|   price|                rank|        similar_item|               tech1|tech2|               title|        price_vector|        price_scaled|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "|0894558358|[089455767X, 0894...|                  []|   Critical Thinking|                  []|                  |[The mind-buildin...|                null|                  []|   |[https://images-n...|[https://images-n...|Industrial & Scie...|   21.99|91,253 in Industr...|                    |                    |     |Science Detective...|[21.989999771118164]|[0.00464956059402...|\n",
      "|1587790637|[158779683X, B004...|[B07BCNHC3K, B078...|Anatomical Chart ...|[Industrial & Sci...|                  |[<DIV><DIV>This c...|                null|                  []|   |                  []|                  []|Industrial & Scie...|59.06999|58,273 in Industr...|                    |                    |     |The Skeletal Syst...| [59.06998825073242]|[0.01263130387988...|\n",
      "|1587791900|                  []|[B07F3MX5JS, B001...|Anatomical Chart ...|[Industrial & Sci...|September 14, 2006|[<div><div>, Our ...|                null|[Brand New. In or...|   |                  []|                  []|     Office Products|   25.35|[\">#102,804 in Of...|                    | class=\"a-keyvalu...|     |The Prostate Anat...|[25.350000381469727]|[0.00537282571388...|\n",
      "|1634061055|[1634061497, B071...|                  []|         Versa Tiles|                  []|                  |[The VersaTiles s...|                null|                  []|   |[https://images-n...|[https://images-n...|Industrial & Scie...|   69.95|123,525 in Indust...|                    |                    |     |VersaTiles Math S...| [69.94999694824219]|[0.01497330666716...|\n",
      "|B0000223SI|[B0000223SK, B000...|                  []|        PORTER-CABLE|[Industrial & Sci...|     April 1, 2004|[Amazon.com This ...|{null, null, null...|[Clog- and loadin...|   |                  []|                  []|Tools & Home Impr...|   15.99|[\">#19,857 in Too...| class=\"a-bordere...| class=\"a-keyvalu...|     |PORTER-CABLE 7400...|[15.989999771118164]|[0.00335801597174...|\n",
      "|B0000223SK|[B0000223SN, B000...|                  []|        PORTER-CABLE|[Industrial & Sci...|                  |[Amazon.com This ...|{null, null, 5.6 ...|[Clog- and loadin...|   |                  []|                  []|Tools & Home Impr...|   17.99|13,586 in Tools &...|                    |                    |     |PORTER-CABLE 7400...|[17.989999771118164]|[0.00378853084583...|\n",
      "|B0000223UV|[B000CSS8UE, B07G...|                  []|             Gorilla|[Industrial & Sci...|      July 7, 2004|[The product that...|{null, null, null...|[Incredibly stron...|   |[https://images-n...|[https://images-n...|Industrial & Scie...|   10.24|[\">#340 in Indust...|                    | class=\"a-keyvalu...|     |Gorilla Original ...|[10.239999771118164]|[0.00212028570872...|\n",
      "|B0000224J0|[B0000224J1, B00N...|                  []|              DEWALT|[Industrial & Sci...|     June 15, 2006|[High performance...|{null, null, null...|[High performance...|   |                  []|                  []|Tools & Home Impr...|    6.98|[\">#51,285 in Too...|                    | class=\"a-keyvalu...|     |DEWALT DW8001 Gen...| [6.980000019073486]|[0.00141854651732...|\n",
      "|B00002N5M6|[B001T4FGS2, B001...|                  []|     Apex Tool Group|                  []|     April 1, 2004|[American pattern...|{null, null, null...|[American pattern...|   |                  []|                  []|Industrial & Scie...|    6.34|[\">#120,776 in In...|                    |                    |     |Nicholson Round H...| [6.340000152587891]|[0.00128078178635...|\n",
      "|B00002N6FE|[B001449TPS, B00O...|                  []|                  3M|[Industrial & Sci...|     June 15, 2006|[3M Garnet Sandpa...|{null, null, null...|[Assorted Grits, ...|   |                  []|                  []|Tools & Home Impr...|    5.41|[\">#3,019 in Tool...|                    | class=\"a-keyvalu...|     |3M Garnet Sandpap...| [5.409999847412109]|[0.00108059230420...|\n",
      "|B00002N6G0|[B000UVP09A, B000...|                  []|                 DAP|[Industrial & Sci...|      July 3, 2006|[DAP traces its r...|{null, null, null...|[Does a better, l...|   |                  []|                  []|Tools & Home Impr...|    6.91|[\">#101,324 in To...|                    | class=\"a-keyvalu...|     |Dap 12121 33 Glaz...| [6.909999847412109]|[0.00140347845978...|\n",
      "|B00002N8T5|[B00004Z4BE, B000...|                  []|                  3M|[Industrial & Sci...|  October 29, 2008|[3M Wood Refinish...|{null, null, null...|[Removes dust, di...|   |                  []|                  []|Tools & Home Impr...|    4.25|[\">#20,302 in Too...|                    | class=\"a-keyvalu...|     | 3M 10132 Tack Cloth|              [4.25]|[8.30893710081047...|\n",
      "|B00002NAOH|                  []|[B00DFQSD5K, B001...|      Gardner Bender|[Industrial & Sci...| December 20, 2001|[Safety is the nu...|{null, null, null...|[Tough ABS constr...|   |[https://images-n...|[https://images-n...|Tools & Home Impr...|    7.41|[\">#182,308 in To...|                    | class=\"a-keyvalu...|     |Gardner Bender RT...| [7.409999847412109]|[0.00151110717830...|\n",
      "|B00002ND49|[B00F1BYJZ0, B075...|                  []|            Shop Vac|[Industrial & Sci...|   January 1, 2007|[For dirt, furnit...|{null, null, null...|[1-1/4, round vac...|   |                  []|                  []|Tools & Home Impr...|    6.83|[\">#23,775 in Too...|                    |                    |     |Shop Vac, 37987, ...| [6.829999923706055]|[0.00138625788123...|\n",
      "|B00002ND6L|[B000BQS5GO, B004...|[B0000CBJI6, B001...|                 DAP|[Industrial & Sci...|   January 1, 2008|[America's #1 sel...|{null, null, null...|[Sturdy and secur...|   |[https://images-n...|[https://images-n...|Tools & Home Impr...|59.06999|[\">#83,620 in Too...|                    | class=\"a-keyvalu...|     |DAP INC 18152 10....| [59.06998825073242]|[0.01263130387988...|\n",
      "|B00002ND4I|[B010CCMXH8, B000...|                  []|            Shop-Vac|[Industrial & Sci...|   August 17, 2005|[Drywall collecti...|{null, null, null...|[N/A, Imported, E...|   |[https://images-n...|[https://images-n...|Tools & Home Impr...|   10.85|[\">#2,338 in Tool...|                    | class=\"a-keyvalu...|     |Shop-Vac 9067200 ...|[10.850000381469727]|[0.00225159287670...|\n",
      "|B00004R9M1|                  []|                  []|         Porta-Nails|[Industrial & Sci...| February 14, 2000|[Accessory for 40...|{null, null, null...|[Pack of 1,000 2-...|   |                  []|                  []|Tools & Home Impr...|59.06999|[\">#3,843,129 in ...|                    | class=\"a-keyvalu...|     |Porta-Nails 1,000...| [59.06998825073242]|[0.01263130387988...|\n",
      "|B00004R9U1|[B0032JTDPO, B000...|                  []|           B&C Eagle|[Industrial & Sci...| February 14, 2000|[B&C Eagle collat...|{null, null, null...|[Compatible with ...|   |                  []|                  []|Tools & Home Impr...|   12.99|[\">#5,260 in Tool...|                    | class=\"a-keyvalu...|     |B&amp;C Eagle A3X...|[12.989999771118164]|[0.00271224366060...|\n",
      "|B00004RHAO|[B00004RHAL, B005...|                  []|              DEWALT|[Industrial & Sci...|  February 4, 2006|[Featuring high-t...|{null, null, null...|[Constructed with...|   |                  []|                  []|Tools & Home Impr...|   12.16|[\">#9,735 in Tool...|                    | class=\"a-keyvalu...|     |DEWALT DW4930 4-I...| [12.15999984741211]|[0.00253358000427...|\n",
      "|B00004RH8I|[B005B8LZCI, B005...|                  []|              DEWALT|[Industrial & Sci...|September 15, 2000|[DEWALT DW4523 4-...|{null, null, null...|[High-performance...|   |                  []|                  []|Tools & Home Impr...|    3.73|[\">#14,608 in Too...|                    | class=\"a-keyvalu...|     |DEWALT DW4523 4-1...|[3.7300000190734863]|[7.18959846922271...|\n",
      "+----------+--------------------+--------------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+---+--------------------+--------------------+--------------------+--------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'price' column into a vector column\n",
    "assembler_for_price = VectorAssembler(inputCols=[\"price\"], outputCol=\"price_vector\")\n",
    "\n",
    "# Normalize the price column\n",
    "price_scaler = MinMaxScaler(inputCol=\"price_vector\", outputCol=\"price_scaled\")\n",
    "\n",
    "# Define the pipeline with all the stages\n",
    "pipeline = Pipeline(stages=[\n",
    "    # brand_indexer,\n",
    "    # brand_encoder,\n",
    "    assembler_for_price,\n",
    "    price_scaler\n",
    "    # assembler_for_features\n",
    "])\n",
    "\n",
    "# After cleaning, try fitting the pipeline again\n",
    "pipeline_model = pipeline.fit(df_train)\n",
    "\n",
    "# Now transform the data\n",
    "df_transformed = pipeline_model.transform(df_train)\n",
    "\n",
    "# Show the result\n",
    "df_transformed.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 08:31:16 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:31:19 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:31:27 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:31:31 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:31:31 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:31:34 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:31:34 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------------+\n",
      "|    reviewerID|         avg_price|         brand|\n",
      "+--------------+------------------+--------------+\n",
      "|A1XSLKDHJVS9WT|3.9800000190734863|    Frost King|\n",
      "|A3LS2HKRVRYK9Q|132.49000549316406|           Jet|\n",
      "| A1M97ZR8OPE3I| 94.81333669026692|           Jet|\n",
      "| A5KBLLLHVL65K| 3.990000009536743|Hy-Ko Products|\n",
      "| AKH1CEX3TGZGP| 3.990000009536743|Hy-Ko Products|\n",
      "|A1LG0RB22GV3QY| 27.16999626159668|        Geocel|\n",
      "| AF3AZVK90T805| 59.06998825073242|      J-B Weld|\n",
      "|A1XKVW78IIGXP0| 59.06998825073242|      J-B Weld|\n",
      "|A2GN2Y3M2Q9XNX| 57.61000061035156|      Shop-Vac|\n",
      "|A2FSU0FOYLVBU3| 57.61000061035156|      Shop-Vac|\n",
      "+--------------+------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 08:31:34 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    }
   ],
   "source": [
    "def calculate_similarity(user_brand, user_avg_price, item_brand, item_price):\n",
    "    # Simple similarity based on brand matching and price difference\n",
    "    brand_similarity = 1 if user_brand == item_brand else 0\n",
    "    # Prevent division by zero if user_avg_price is 0\n",
    "    price_similarity = 1 - abs(item_price - user_avg_price) / user_avg_price if user_avg_price != 0 else 0\n",
    "    # Combine the two components of similarity\n",
    "    return brand_similarity * 0.5 + price_similarity * 0.5\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, count, col, broadcast\n",
    "\n",
    "df_train = df_transformed\n",
    "\n",
    "# Prepare the item profiles from the training set\n",
    "item_profiles_train = df_train.select('asin', 'brand', 'price').distinct()\n",
    "\n",
    "# Calculate the average price for each user in the training set\n",
    "avg_price_train = train.join(df_train, 'asin').groupBy('reviewerID').agg(avg('price').alias('avg_price'))\n",
    "\n",
    "# Count the occurrences of each brand for each user in the training set and find the most frequent one\n",
    "brand_frequency_train = train.join(df_train, 'asin').groupBy('reviewerID', 'brand').agg(count('brand').alias('brand_count'))\n",
    "\n",
    "# Find the top brand for each user in the training set\n",
    "windowSpec = Window.partitionBy('reviewerID').orderBy(col('brand_count').desc(), col('brand'))\n",
    "top_brands_train = (brand_frequency_train.withColumn('rank', F.row_number().over(windowSpec))\n",
    "                    .where(col('rank') == 1)\n",
    "                    .select('reviewerID', 'brand'))\n",
    "\n",
    "# Create the user profiles using the training set\n",
    "user_profiles_train = avg_price_train.join(broadcast(top_brands_train), 'reviewerID')\n",
    "\n",
    "user_profiles_train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 08:32:13 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:32:24 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:32:24 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "[Stage 67:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+\n",
      "|      asin|               brand| price|\n",
      "+----------+--------------------+------+\n",
      "|B00004SZ5J|         Irwin Tools| 64.59|\n",
      "|B00004T7W2|General Tools Mfg...|  5.58|\n",
      "|B00004XPVN|              Wilton|122.92|\n",
      "|B00004Z2HD|      ARROW FASTENER|  2.46|\n",
      "|B00007JQQI|       Master Caster|   6.2|\n",
      "|B00008IHTK|            Starrett|268.93|\n",
      "|B0002XRM4W|     ISC Racers Tape| 15.45|\n",
      "|B0002YUNRE|      Gardner Bender|  5.45|\n",
      "|B0002YWNLS|               Eagle|  4.59|\n",
      "|B0008JIQVI|            Shop-Vac| 57.61|\n",
      "+----------+--------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 08:32:27 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "item_profiles_train.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 08:38:18 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:38:18 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:38:21 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:38:30 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:38:33 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:38:33 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:38:36 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:38:36 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:38:36 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+------------------+\n",
      "|    reviewerID|         avg_price|             brand|\n",
      "+--------------+------------------+------------------+\n",
      "|A32FZPORDRP3A3| 4.949999809265137|          BOSTITCH|\n",
      "| AHMI8DQJJMB49|17.309999465942383|             Omron|\n",
      "| ARH2FYMMOXFB5| 9.260000228881836| The Hillman Group|\n",
      "|A1AUYOLJHUBFY1|21.427997636795045|    Gardner Bender|\n",
      "| ARM3HEO1D67NF| 5.710000038146973|       Install Bay|\n",
      "|A29QEUREQ9OMAA| 59.06998825073242|       Small Parts|\n",
      "| AY68U1SJ0UB8B|  4.46999979019165|        Quakehold!|\n",
      "|A3R1N4BRAH1S61| 67.44499969482422|   MDF Instruments|\n",
      "| A3SWMTPJNHWUM|10.010000228881836|           Dynarex|\n",
      "|A30VD61TSFUZVJ| 59.06998825073242|           Unknown|\n",
      "|A1K344MWYUQ24A|15.699999809265137|               ADC|\n",
      "| A1XQZ84K9JCGL|137.23000462849936|             Fluke|\n",
      "|A2Y64L1IFG2MVH| 59.06998825073242|               ARB|\n",
      "|A119TAKX9W6FLM| 59.06998825073242|             Valeo|\n",
      "|A20YQUOOUKMEYM| 59.06998825073242|   MDF Instruments|\n",
      "|A1AGCZQPZ9PZNR| 59.06998825073242|   MDF Instruments|\n",
      "|A33FFJYL7FKRIA| 9.460000038146973|            EyeVac|\n",
      "| AI3TBZDKX59YQ| 9.460000038146973|            EyeVac|\n",
      "|A1537HLRO240XP|13.279999732971191|       Irwin Tools|\n",
      "|A3P7YTDBD7LQPR|13.989999771118164|Learning Resources|\n",
      "+--------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 08:38:37 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    }
   ],
   "source": [
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2p1JQ8vRJ_4-",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Register the UDF\n",
    "similarity_udf = F.udf(calculate_similarity, FloatType())\n",
    "\n",
    "# Generate recommendations for users in the validation set\n",
    "# First, get the list of items each user has reviewed in the training set to exclude them from recommendations\n",
    "df_already_reviewed_train = train.select('reviewerID', 'asin').distinct().alias(\"ar_train\")\n",
    "\n",
    "# Calculate similarities between user profiles and item profiles, excluding already reviewed items\n",
    "user_profiles_val = test.select('reviewerID').distinct().alias(\"up_val\")\n",
    "item_profiles_alias = item_profiles_train.alias(\"ip\")\n",
    "\n",
    "user_profiles_val = user_profiles_val.join(broadcast(user_profiles_train), \"reviewerID\", \"inner\")\n",
    "user_profiles_val = user_profiles_val.alias(\"up_val\")\n",
    "user_item_similarity = (user_profiles_val\n",
    "                        .crossJoin(broadcast(item_profiles_alias))\n",
    "                        .withColumn('similarity', similarity_udf(\n",
    "                            col('up_val.brand'), col('up_val.avg_price'), col('ip.brand'), col('ip.price')))\n",
    "                        .select('up_val.reviewerID', 'ip.asin', 'similarity'))\n",
    "\n",
    "recommendations = (user_item_similarity.join(df_already_reviewed_train,\n",
    "                                             (user_item_similarity.reviewerID == df_already_reviewed_train.reviewerID) &\n",
    "                                             (user_item_similarity.asin == df_already_reviewed_train.asin), 'left_anti')\n",
    "                   .groupBy('reviewerID')\n",
    "                   .agg(expr('collect_list(struct(asin, similarity)) as recommendations')))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 08:40:56 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:40:56 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:40:59 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:07 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:07 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:10 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:10 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:11 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:13 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:13 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:14 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "23/11/08 08:41:14 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+----------+\n",
      "|    reviewerID|      asin|similarity|\n",
      "+--------------+----------+----------+\n",
      "|A32FZPORDRP3A3|B00004SZ5J|-5.5242424|\n",
      "|A32FZPORDRP3A3|B00004T7W2| 0.4363636|\n",
      "|A32FZPORDRP3A3|B00004XPVN|-11.416162|\n",
      "|A32FZPORDRP3A3|B00004Z2HD|0.24848486|\n",
      "|A32FZPORDRP3A3|B00007JQQI|0.37373737|\n",
      "|A32FZPORDRP3A3|B00008IHTK|-26.164646|\n",
      "|A32FZPORDRP3A3|B0002XRM4W|-0.5606061|\n",
      "|A32FZPORDRP3A3|B0002YUNRE|0.44949496|\n",
      "|A32FZPORDRP3A3|B0002YWNLS| 0.4636364|\n",
      "|A32FZPORDRP3A3|B0008JIQVI|-4.8191924|\n",
      "+--------------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_item_similarity.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 08:41:15 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:15 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:15 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:18 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:26 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:26 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:26 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:29 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:29 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:30 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:33 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:33 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 08:41:33 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "23/11/08 08:41:33 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "23/11/08 08:41:56 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|    reviewerID|     recommendations|\n",
      "+--------------+--------------------+\n",
      "|A10AY74351AJSG|[{B00004SZ5J, 0.4...|\n",
      "|A10O7THJ2O20AG|[{B00004SZ5J, -0....|\n",
      "|A10SSEK4KB0SMD|[{B00004SZ5J, 0.4...|\n",
      "|A10Z8K7CVE3VHZ|[{B00004SZ5J, -5....|\n",
      "|A10ZFE6YE0UHW8|[{B00004SZ5J, -0....|\n",
      "|A119TAKX9W6FLM|[{B00004SZ5J, 0.4...|\n",
      "|A11JI4CX8C84OT|[{B00004SZ5J, -3....|\n",
      "|A12EEGO26GHBAY|[{B00004SZ5J, 0.4...|\n",
      "|A12JE5QXN10IO1|[{B00004SZ5J, -3....|\n",
      "|A132JF54KQKAKH|[{B00004SZ5J, -2....|\n",
      "+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recommendations.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sort and get top N recommendations for each user\n",
    "top_n = 1  # Assuming we want the top 1 recommendations\n",
    "get_top_n = F.udf(lambda recs: sorted(recs, key=lambda x: x[1], reverse=True)[:top_n], ArrayType(StructType()))\n",
    "final_recommendations = (recommendations\n",
    "                         .withColumn('top_recommendations', get_top_n(col('recommendations')))\n",
    "                         .select('reviewerID', 'top_recommendations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/08 09:02:14 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:14 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:14 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:17 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:25 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:25 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:26 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:29 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:29 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:29 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:32 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:32 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "23/11/08 09:02:32 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "23/11/08 09:02:32 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "23/11/08 09:02:53 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "23/11/08 09:02:54 ERROR Executor: Exception in task 0.0 in stage 194.0 (TID 118)\n",
      "java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 0 fields are required while 2 values are provided.\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.$anonfun$applyOrElse$1(EvaluatePython.scala:162)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.applyOrElse(EvaluatePython.scala:162)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$13(EvaluatePython.scala:160)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$7(BatchEvalPythonExec.scala:101)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage23.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/11/08 09:02:54 WARN TaskSetManager: Lost task 0.0 in stage 194.0 (TID 118) (udc-aw34-17c0 executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 0 fields are required while 2 values are provided.\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.$anonfun$applyOrElse$1(EvaluatePython.scala:162)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.applyOrElse(EvaluatePython.scala:162)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n",
      "\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$13(EvaluatePython.scala:160)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$7(BatchEvalPythonExec.scala:101)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage23.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/11/08 09:02:54 ERROR TaskSetManager: Task 0 in stage 194.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1060.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 194.0 failed 1 times, most recent failure: Lost task 0.0 in stage 194.0 (TID 118) (udc-aw34-17c0 executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 0 fields are required while 2 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.$anonfun$applyOrElse$1(EvaluatePython.scala:162)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.applyOrElse(EvaluatePython.scala:162)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$13(EvaluatePython.scala:160)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$7(BatchEvalPythonExec.scala:101)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage23.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 0 fields are required while 2 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.$anonfun$applyOrElse$1(EvaluatePython.scala:162)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.applyOrElse(EvaluatePython.scala:162)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$13(EvaluatePython.scala:160)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$7(BatchEvalPythonExec.scala:101)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage23.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Show the final recommendations\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfinal_recommendations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/apps/software/standard/core/pyspark/3.4.1-py3.11/lib/python3.11/site-packages/pyspark/sql/dataframe.py:912\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    905\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    906\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m         },\n\u001b[1;32m    910\u001b[0m     )\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/apps/software/standard/core/pyspark/3.4.1-py3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/apps/software/standard/core/pyspark/3.4.1-py3.11/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/apps/software/standard/core/pyspark/3.4.1-py3.11/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1060.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 194.0 failed 1 times, most recent failure: Lost task 0.0 in stage 194.0 (TID 118) (udc-aw34-17c0 executor driver): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 0 fields are required while 2 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.$anonfun$applyOrElse$1(EvaluatePython.scala:162)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.applyOrElse(EvaluatePython.scala:162)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$13(EvaluatePython.scala:160)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$7(BatchEvalPythonExec.scala:101)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage23.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:382)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:354)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3161)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3382)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:323)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 0 fields are required while 2 values are provided.\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:188)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:182)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.$anonfun$applyOrElse$1(EvaluatePython.scala:162)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$13$1.applyOrElse(EvaluatePython.scala:162)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:213)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$13(EvaluatePython.scala:160)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$7(BatchEvalPythonExec.scala:101)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage23.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Show the final recommendations\n",
    "final_recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
